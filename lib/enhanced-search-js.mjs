/**
 * å¢å¼ºæœç´¢ç´¢å¼•ç”Ÿæˆå™¨ - JavaScriptç‰ˆæœ¬
 * ä¾›æ„å»ºè„šæœ¬ä½¿ç”¨
 */

import crypto from 'crypto'
import { writeFileSync } from 'fs'

import { calculatePostWeight, getTotalContentBlocks } from './search/utils.ts'

// ç®€åŒ–çš„ä¸­æ–‡åˆ†è¯å™¨
class SimpleTokenizer {
  static tokenize(text) {
    const tokens = []
    const cleanText = text
      .replace(/\s+/g, ' ')
      .replace(/[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹]/g, ' ')
      .trim()

    // åˆ†ç¦»ä¸­è‹±æ–‡
    const segments = this.splitMixedText(cleanText)

    for (const segment of segments) {
      if (this.isChinese(segment)) {
        tokens.push(...this.tokenizeChinese(segment))
      } else {
        tokens.push(...this.tokenizeEnglish(segment))
      }
    }

    return tokens.filter(token => token.length > 0)
  }

  static splitMixedText(text) {
    const segments = []
    let current = ''
    let isChinese = false

    for (let i = 0; i < text.length; i++) {
      const char = text[i]
      const charIsChinese = /[\u4e00-\u9fff]/.test(char)

      if (i === 0) {
        isChinese = charIsChinese
        current = char
      } else if (charIsChinese === isChinese) {
        current += char
      } else {
        if (current.trim()) {
          segments.push(current.trim())
        }
        current = char
        isChinese = charIsChinese
      }
    }

    if (current.trim()) {
      segments.push(current.trim())
    }

    return segments
  }

  static isChinese(text) {
    return /[\u4e00-\u9fff]/.test(text)
  }

  static tokenizeChinese(text) {
    const tokens = []

    // å•å­—åˆ†è¯
    for (const char of text) {
      if (char.trim() && /[\u4e00-\u9fff]/.test(char)) {
        tokens.push(char)
      }
    }

    // äºŒå­—è¯ç»„
    for (let i = 0; i < text.length - 1; i++) {
      const bigram = text.slice(i, i + 2)
      if (bigram.length === 2 && /^[\u4e00-\u9fff]{2}$/.test(bigram)) {
        tokens.push(bigram)
      }
    }

    // ä¸‰å­—è¯ç»„
    for (let i = 0; i < text.length - 2; i++) {
      const trigram = text.slice(i, i + 3)
      if (trigram.length === 3 && /^[\u4e00-\u9fff]{3}$/.test(trigram)) {
        tokens.push(trigram)
      }
    }

    return tokens
  }

  static tokenizeEnglish(text) {
    return text
      .toLowerCase()
      .split(/\s+/)
      .filter(word => word.length > 0 && /^[a-zA-Z0-9]+$/.test(word))
  }
}

// ç®€åŒ–çš„å†…å®¹è§£æå™¨
class SimpleContentParser {
  static parseContent(rawContent) {
    const contents = []
    const lines = rawContent.split('\n')
    let position = 0

    for (let i = 0; i < lines.length; i++) {
      const line = lines[i].trim()
      if (!line) continue

      const content = this.createSearchableContent(line, position++)
      if (content) {
        contents.push(content)
      }
    }

    return contents
  }

  static createSearchableContent(line, position) {
    // è·³è¿‡å‰ç½®å†…å®¹å’Œæ³¨é‡Š
    if (line.startsWith('---') || line.startsWith('import') || line.startsWith('//')) {
      return null
    }

    // å¤„ç†æ ‡é¢˜
    const headingMatch = line.match(/^(#{1,6})\s+(.+)/)
    if (headingMatch) {
      const level = headingMatch[1].length
      const content = headingMatch[2]
      const plainText = this.stripMarkdown(content)

      return {
        id: `heading-${position}`,
        type: 'heading',
        content,
        plainText,
        tokens: SimpleTokenizer.tokenize(plainText),
        position,
        level,
      }
    }

    // å¤„ç†ä»£ç å—å¼€å§‹
    const codeBlockMatch = line.match(/^```(\w+)?/)
    if (codeBlockMatch) {
      return {
        id: `code-${position}`,
        type: 'code',
        content: line,
        plainText: line,
        tokens: SimpleTokenizer.tokenize(line),
        position,
        language: codeBlockMatch[1] || 'text',
      }
    }

    // å¤„ç†åˆ—è¡¨é¡¹
    const listMatch = line.match(/^[\s]*[-*+]\s+(.+)/)
    if (listMatch) {
      const content = listMatch[1]
      const plainText = this.stripMarkdown(content)

      return {
        id: `list-${position}`,
        type: 'list',
        content,
        plainText,
        tokens: SimpleTokenizer.tokenize(plainText),
        position,
      }
    }

    // å¤„ç†å¼•ç”¨
    const quoteMatch = line.match(/^>\s+(.+)/)
    if (quoteMatch) {
      const content = quoteMatch[1]
      const plainText = this.stripMarkdown(content)

      return {
        id: `quote-${position}`,
        type: 'quote',
        content,
        plainText,
        tokens: SimpleTokenizer.tokenize(plainText),
        position,
      }
    }

    // å¤„ç†æ™®é€šæ®µè½
    if (line.length > 10) {
      const plainText = this.stripMarkdown(line)

      return {
        id: `paragraph-${position}`,
        type: 'paragraph',
        content: line,
        plainText,
        tokens: SimpleTokenizer.tokenize(plainText),
        position,
      }
    }

    return null
  }

  static stripMarkdown(text) {
    return text
      .replace(/\*\*(.*?)\*\*/g, '$1')
      .replace(/\*(.*?)\*/g, '$1')
      .replace(/__(.*?)__/g, '$1')
      .replace(/_(.*?)_/g, '$1')
      .replace(/`(.*?)`/g, '$1')
      .replace(/\[([^\]]+)\]\([^)]+\)/g, '$1')
      .replace(/!\[([^\]]*)\]\([^)]+\)/g, '$1')
      .replace(/[#*>-]/g, '')
      .trim()
  }
}

// ä¸»è¦çš„å¢å¼ºç´¢å¼•ç”Ÿæˆå‡½æ•°
export async function createEnhancedSearchIndexJS(posts) {
  console.log('ğŸ” å¼€å§‹ç”Ÿæˆå¢å¼ºæœç´¢ç´¢å¼•ï¼ˆJSç‰ˆæœ¬ï¼‰...')
  const startTime = Date.now()

  const searchIndices = []
  let processedCount = 0
  const totalCount = posts.filter(
    post => !post.draft || process.env.NODE_ENV !== 'production',
  ).length

  for (const post of posts) {
    if (post.draft && process.env.NODE_ENV === 'production') continue

    console.log(`ğŸ“„ å¤„ç†æ–‡ç« : ${post.title} (${++processedCount}/${totalCount})`)

    try {
      // è§£ææ–‡ç« å†…å®¹
      const contents = SimpleContentParser.parseContent(post.body.raw)

      // è®¡ç®—æ–‡ç« æƒé‡
      const weight = calculatePostWeight(post)

      // åˆ›å»ºå¢å¼ºç´¢å¼•
      const searchIndex = {
        slug: post.slug,
        title: post.title,
        summary: post.summary || '',
        tags: post.tags || [],
        date: post.date,
        lastmod: post.lastmod,
        author: post.author,
        readingTime: {
          text: post.readingTime?.text || '1 min read',
          minutes: post.readingTime?.minutes || 1,
          words: post.readingTime?.words || 100,
        },
        contents,
        toc: (post.toc || []).map((item, index) => ({
          ...item,
          position: index,
        })),
        weight,
      }

      searchIndices.push(searchIndex)
    } catch (error) {
      console.error(`âŒ å¤„ç†æ–‡ç« å¤±è´¥: ${post.title}`, error)
      // åˆ›å»ºåŸºç¡€ç´¢å¼•ä½œä¸ºå›é€€
      const fallbackIndex = {
        slug: post.slug,
        title: post.title,
        summary: post.summary || '',
        tags: post.tags || [],
        date: post.date,
        lastmod: post.lastmod,
        author: post.author,
        readingTime: {
          text: '1 min read',
          minutes: 1,
          words: 100,
        },
        contents: [
          {
            id: 'fallback-0',
            type: 'paragraph',
            content: post.body.raw.slice(0, 500),
            plainText: post.body.raw.slice(0, 500),
            position: 0,
          },
        ],
        toc: [],
        weight: 1,
      }
      searchIndices.push(fallbackIndex)
    }
  }

  // æŒ‰æƒé‡å’Œæ—¥æœŸæ’åº
  searchIndices.sort((a, b) => {
    const weightDiff = b.weight - a.weight
    if (Math.abs(weightDiff) > 0.1) return weightDiff
    return new Date(b.date).getTime() - new Date(a.date).getTime()
  })

  // ç”Ÿæˆå¤šä¸ªç´¢å¼•æ–‡ä»¶
  await generateSearchIndices(searchIndices)

  const endTime = Date.now()
  console.log(`âœ… å¢å¼ºæœç´¢ç´¢å¼•ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: ${endTime - startTime}ms`)
  console.log(
    `ğŸ“Š å¤„ç†äº† ${searchIndices.length} ç¯‡æ–‡ç« ï¼Œå…± ${getTotalContentBlocks(searchIndices)} ä¸ªå†…å®¹å—`,
  )
}

async function generateSearchIndices(indices) {
  // 1. ç”Ÿæˆå®Œæ•´çš„å¢å¼ºç´¢å¼•
  writeFileSync('public/search-enhanced.json', JSON.stringify(indices, null, 0))
  console.log('ğŸ“ ç”Ÿæˆå¢å¼ºæœç´¢ç´¢å¼•: search-enhanced.json')

  // 2. æ›´æ–°å…¼å®¹çš„ç®€åŒ–ç´¢å¼•
  const simpleIndices = indices.map(index => ({
    slug: index.slug,
    title: index.title,
    summary: index.summary,
    content: index.contents
      .map(c => c.plainText)
      .join(' ')
      .slice(0, 1000),
    tags: index.tags,
    date: index.date,
  }))
  writeFileSync('public/search.json', JSON.stringify(simpleIndices, null, 0))
  console.log('ğŸ“ æ›´æ–°å…¼å®¹æœç´¢ç´¢å¼•: search.json')

  // 3. ç”Ÿæˆå…ƒæ•°æ®ç´¢å¼•
  const metaIndices = indices.map(index => ({
    slug: index.slug,
    title: index.title,
    summary: index.summary,
    tags: index.tags,
    date: index.date,
    readingTime: index.readingTime,
    weight: index.weight,
    contentCount: index.contents.length,
  }))
  writeFileSync('public/search-meta.json', JSON.stringify(metaIndices, null, 0))
  console.log('ğŸ“ ç”Ÿæˆå…ƒæ•°æ®ç´¢å¼•: search-meta.json')

  // 4. ç”Ÿæˆå…ƒæ•°æ®ä¿¡æ¯
  const metadata = {
    version: Date.now(),
    totalPosts: indices.length,
    lastUpdate: new Date().toISOString(),
    buildId: crypto.randomBytes(8).toString('hex'),
  }
  writeFileSync('public/search-metadata.json', JSON.stringify(metadata, null, 2))
  console.log('ğŸ“ ç”Ÿæˆæœç´¢å…ƒæ•°æ®: search-metadata.json')
}
